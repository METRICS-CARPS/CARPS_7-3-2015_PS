---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r}
articleID <- "7-3-2015_PS" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- 'pilot'
pilotNames <- "Danielle Boles, Michael Ko" # insert the pilot's name here e.g., "Tom Hardwicke". If there are multiple pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "" # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 150 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- NA # insert the co- pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("10/27/17", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

-------

#### Methods summary: 
Participants (N=21) completed a series of trials that required them to switch or stay from one task to the other. One task was to choose the larger value of the two values if surrounded by a green box. The other task was to choose the value with the larger font if surrounded by a blue box. Subliminal cues followed by a mask were presented before each trial. Cues included "O" (non-predictive cue), "M" (switch predictive cue), and "T" (repeat predictive cue). Reaction times and performance accuracy were measured.

------

#### Target outcomes: 
> Performance on switch trials, relative to repeat trials, incurred a switch cost that was evident in longer RTs (836
vs. 689 ms) and lower accuracy rates (79% vs. 92%). If
participants were able to learn the predictive value of the
cue that preceded only switch trials and could instantiate
relevant anticipatory control in response to it, the performance
on switch trials preceded by this cue would be
better than on switch trials preceded by the nonpredictive
cue. This was indeed the case (mean RT-predictive
cue: 819 ms; nonpredictive cue: 871 ms; mean difference
= 52 ms, 95% confidence interval, or CI = [19.5,
84.4]), two-tailed paired t(20) = 3.34, p < .01. However,
error rates did not differ across these two groups of switch trials (predictive cue: 78.9%; nonpredictive cue: 78.8%), p = .8.

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(broom)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared.
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

## Step 2: Load data

```{r warning=FALSE}
# This reads all the particiapnts data (each is in a seperase xls file) in and combines them into one dataframe
# Each xls has 250 rows, the rest is their calculations using excel, which we don't want in the data
files <- dir('data/Experiment 1')

data <- data.frame()
id <- 1
for (file in files){
  if(file != 'Codebook.xls'){
    temp_data <- read_xls(file.path('data/Experiment 1', file))
    temp_data$id <- id
    id <- id + 1
    temp_data <- temp_data[1:250, ]
    data <- rbind(data, temp_data)
  }
}
```

## Step 3: Tidy data

Each row is an observation. The data is already in tidy.

```{r}
# each trial is an observation
# the data is in tidy already
```

## Step 4: Run analysis

### Pre-processing

The codebook for Experiment 1 listed O, T, and M as the only primes that they used. However, we found that some participants had primes 2, 4, and 8 instead.
We inferred that 2 is the nonpredictive cue, 4 is the repeat predictive cue and 8 as the switch predictive cue based on how the other columns were named. 
Thus we will proceed the analysis with this assumption by recoding the primes this way.

```{r recode}
data$Prime <- recode(data$Prime, '2' = "O", '4' = "T", '8' = "M")

#recode variables to make referencing easier
data$Prime <- recode(data$Prime, 'O' = "Nonpredictive Cue", 'M' = "Switch Predictive Cue", 'T' = "Repeat Predictive Cue")
data$TrialType <- recode(data$TrialType, '0' = "Neither", '1' = "Repeat Trials", '2' = "Switch Trials")


```

### Descriptive statistics

We will first try to reproduce median reaction time of switch trials and non switch trials.

> Performance on switch trials, relative to repeat trials,
incurred a switch cost that was evident in longer RTs (836
vs. 689 ms)

We used median as the author's instructed to use median for reaction time unless otherwise reported.

```{r median_RT}
med_RT <- data %>% 
        group_by(TrialType) %>% 
        summarise(median_RT = median(RT))

kable(med_RT[-1, ])

```

These values are slightly off. There is ambiguity in how to calculate this statistics which is detailed below. 

[INSUFFICIENT INFORMATION ERROR]

```{r median_RT diff}
reportObject <- reproCheck(reportedValue = "836", obtained = 812.94, valueType = 'median')
reportObject <- reproCheck(reportedValue = "689", obtained = 665.06, valueType = 'median')
```

-----------------

Next we will try to reproduce the accuracy of switch trials and non switch trials.

> Performance on switch trials, relative to repeat trials,
incurred a switch cost that was evident in [...] lower 
accuracy rates (79% vs. 92%)


```{r mean_Correct_Response}
mean_RespCorr <- data %>% 
        group_by(TrialType) %>%
        summarise(accuracy = mean(RespCorr))

kable(mean_RespCorr[-1, ])


```

These values match.

--------------------

Now we will analyze Predicitve Switch Cues vs Nonpredictive Switch Cues. Let's start with reaction time.

> This was indeed the case (mean RT-predictive
cue: 819 ms; nonpredictive cue: 871 ms; ... )

Later the authors do a t test with 20 as the degrees of freedoms. So we will 
assume that these mean values come from individual medians of RT.

```{r mean Prime RT}
mean_Prime_RT_Ind <- data %>% filter(TrialType == "Switch Trials") %>% group_by(id, Prime) %>%summarise(meanRT = mean(RT)) #Individual Means
mean_Prime_RT <- mean_Prime_RT_Ind %>% group_by(Prime) %>% summarise(grandmeanRT = mean(meanRT)) #Grand Means

kable(mean_Prime_RT)
```

These numbers doesn't match. There is ambiguity on how to calculate this statstic which is detailed below.

[INSUFFICIENT INFORMATION ERROR]

```{r mean Prime RT diff}
reportObject <- reproCheck(reportedValue = "819", obtained = 883.4, valueType = 'mean')
reportObject <- reproCheck(reportedValue = "871", obtained = 907.8, valueType = 'mean')
```

---------------------------

Next we will try to reproduce accuracy of switch predicitve cues vs switch nonpredictive cues.

> However, error rates did not differ across these two 
groups of switch trials (predictive cue: 78.9%; nonpredictive cue: 78.8%)

Again, we will asssume that the accuracy statistic was calculated from the mean of each individual participant's accuracy.

```{r Prime Accuracy}
mean_Prime_RespCorr_Ind <- data %>% filter(TrialType == "Switch Trials") %>% group_by(id, Prime) %>% summarise(meanCorr = mean(RespCorr)) #Individual Means
mean_Prime_RespCorr <- mean_Prime_RespCorr_Ind %>% group_by(Prime) %>% summarise(grandmeanCorr = mean(meanCorr)) #Grand Means

kable(mean_Prime_RespCorr)
```

These numbers are very close to the reported numbers. The difference is 
probably due to rounding. We will note this difference anyways.

```{r Prime Accuracy Diff}
reportObject <- reproCheck(reported = ".789", obtained = .7994, valueType = 'mean')
reportObject <- reproCheck(reported = ".788", obtained = .7803, valueType = 'mean')
```

### Inferential statistics

The first claim is that in switch trials, predictive cues lead to statistically significant faster reaction times 
than nonpredictive cues.

> ... the performance on switch trials preceded by this cue would be
better than on switch trials preceded by the nonpredictive
cue. This was indeed the case (mean RT-predictive
cue: 819 ms; nonpredictive cue: 871 ms; mean difference
= 52 ms, 95% confidence interval, or CI = [19.5,
84.4]), two-tailed paired t(20) = 3.34, p < .01.

```{r Prime RT test}
mean_Prime_RT_Ind <- mean_Prime_RT_Ind %>% spread(Prime, meanRT) #spreading so that the cues are easier to compare
test <- t.test(mean_Prime_RT_Ind[['Nonpredictive Cue']], mean_Prime_RT_Ind[['Switch Predictive Cue']], paired = T) 

kable(tidy(test))
```

We do not find the same p value as the original paper. There is ambiguity in how to calculate this statistic which is detailed below.

[INSUFFICIENT INFORMATION ERROR]

-----------------

Next we will test their second claim.

> However, error rates did not differ across these two groups of 
switch trials (predictive cue: 78.9%; nonpredictive cue: 78.8%), p = .8.

```{r mean Prime accuracy test}
mean_Prime_RespCorr_Ind <- mean_Prime_RespCorr_Ind %>% spread(Prime, meanCorr) #spreading so that the cues are easier to compare
test <- t.test(mean_Prime_RespCorr_Ind[['Nonpredictive Cue']], mean_Prime_RespCorr_Ind[['Switch Predictive Cue']], paired = TRUE)

kable(tidy(test))
```

Although still insignificant, the p value is very different from what was reported.

```{r mean Prime accuracy test diff}
reportObject <- reproCheck(reportedValue = ".8", obtained = .2945, valueType = 'p')
```


## Step 5: Conclusion

This reproducibility check was a failure. Generally, all the reaction time statistics (even the means) were different from what was reported. This failure can be attributed to a number of reasons.

**Poor name choice for variables**

There is a variable "CorrResp" (1 or 0) and another variable "RespCorr" (TRUE or FALSE). We used "RespCorr" 
because it was the only variable of the two that was included in the codebook where TRUE=Accurate response 
and FALSE=Error. But we still don't know what "CorrResp" is and whether or not they used it in the analyses.

**Unclear recoding of variables**

In the data file, there is 1 excel file per participant with all of their reaction times to the 250 trials. 
For some participants, the Prime was coded as the actual prime shown "O", "T", or "M". For other participants, 
the Prime was coded as "2", "4", and "8". However, we had to infer which number corresponded to each letter by 
looking at the variables names assigned to trial type and which cue followed ("stay_2", "stay_4", "swt_2", "swt_8"). 
We coded 2=O, 4=T, 8=M, but still unsure whether these are consistent with how the authors coded the prime variable.

Also, it is unclear how "RespCorr" is coded - does "TRUE" reflect an accurate response for all blocks? The authors 
noted that for the first 50 trials, responses just had to be correct to be considered an accurate response. 
Thereafter, responses had to be both correct and fast (accurate but slow responses would be considered an error),
with the threshold for speed placed at the 60th percentile for that participants' responses on the first 50 trials. 
We did not calculate the 60th percentile threshold to see if the "RespCorr" was coded correctly, though we did 
yield the same accuracy rates between switch and repeat trials.

**Ambiguity between using means or medians**

The authors noted that unless otherwise noted, statistical tests were performed on median values rather than mean 
values. We followed this according to the paper's protocol. However, we're not able to reproduce the following 
findings using either means or medians for the following findings:

* "performance on switch trials, relative to repeat trials, incurred a switch cost that was evident in longer RTs (836 vs. 689 ms)"
        
* "mean RT - predictive cue: 819 ms; 95% confidence interval, or CI = [19.5, 84.4], two-tailed paired t(20) = 3.34, p < .01"
        
**Unclear whether descriptives of means/medians of means/medians of individuals, or means/medians across all trials**

This point was perhaps the most frustrating in analyzing reaction time. When we tried to reproduce reaction time medians, we realized that the value could have been obtained by calculating a value (mean or median) for each individual, then summarizing those 
values to one value (mean or median), OR it could have been obtained by a value (mean or median) across ALL trials. We 
tried a host of combinations of means or medians with across individuals or across trials, and still could not
replicate the descriptive reaction times.

**Exclusion Criteria**

What data points did the author's exclude? It is not clear after reading this paragraph:

>If participants were able to learn the predictive value of the cue that preceded only switch 
trials and could instantiate relevant anticipatory control in response to it,

We don't know how the author's operationalized "learning the predictive value". Does that mean 
excluding trials with incorrect responses? Excluding participants with too many incorrect responses?
Neither the codebook nor the paper provided sufficient information to account for this statement.

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 3 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add the articleID 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome != "MATCH") | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

## Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```

